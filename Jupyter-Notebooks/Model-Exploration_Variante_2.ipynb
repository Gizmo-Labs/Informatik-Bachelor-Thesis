{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bachelor Thesis - Informatik\n",
    "\n",
    "### Variante 2 :\n",
    "### Multiclass-Klassifizierung mit 2D Convolutional Neuronal Network\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"left\">\n",
    "<h2><img src=\"https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ2hrOGtveWx0ZGxsaGFsYXNzbjVhOTNyaTg2cG44anZ5eWd5a3ZyMyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l3vRcrVqhBVSpJte0/giphy.gif\" width=\"200\" alt=\"Data\"></h2>\n",
    "</div>\n",
    "\n",
    "Dieser Ansatz verwendet als Datengrundlage den folgenden Datensatz :\n",
    "\n",
    "[https://github.com/michidk/myo-dataset]\n",
    "\n",
    "###### ☑️ Der Datensatz wurde neu aufbereitet, siehe dazu --> [Link](Preprocessing.ipynb)\n",
    "\n",
    "###### ☑️ 10 Testpersonen, nachfolgend Subjekte genannt, haben in 6 Sitzungen jeweils 10 Wiederholungen ausgeführt\n",
    "\n",
    "###### ☑️ Alle Personen haben die folgenden drei Gesten in gleicher Position ausgeführt :\n",
    "\n",
    "<img src=\"./Images/paper.png\" width=\"150\">         <img src=\"./Images/rock.png\" width=\"150\">        <img src=\"./Images/scissors.png\" width=\"150\">      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQZWkqdN6PR7"
   },
   "source": [
    "##### ▶️ Installation notwendiger Python-Bibliotheken\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#      Dieses Notebook wurde mit Python 3.9.13 getestet      #\n",
    "#                                                            #\n",
    "#           Andere Versionen haben teilweise                 #\n",
    "#           Kompatbilitätsprobleme verursacht                #\n",
    "#                                                            #\n",
    "#      This Notebook was tested with Python 3.9.13           #\n",
    "#                                                            #\n",
    "#              Other Versions can cause                      #\n",
    "#                Compatibilityproblems                       #\n",
    "#                                                            #\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Achtung!! Die Konvertierung des Modells nach C++           #\n",
    "#           funktioniert nur mit Tensorflow <=2.15.1         #\n",
    "#           Keras 3 funktioniert nicht!                      #\n",
    "#                                                            #\n",
    "# Attention! Converting the model to C++ ist only working    #\n",
    "#            with Tensorflow <=2.15.1                        #\n",
    "#            Keras 3 will not work!                          #\n",
    "##############################################################\n",
    "\n",
    "#%pip install -r requirements.txt\n",
    "#%pip install -q \"eloquent-tensorflow==1.0.5\" \"keras<3\" embedded_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ▶️ Importieren notwendiger Python-Bibliotheken\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNjJFQ0Y_qRl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.metrics import classification_report, confusion_matrix # type: ignore\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler # type: ignore\n",
    "\n",
    "import tensorflow as tf # type: ignore\n",
    "from tensorflow import keras # type: ignore\n",
    "from keras import layers # type: ignore\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ▶️ Einlesen der aufgezeichneten Daten aus dem Quellordner<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US2Lj9nc_MkG"
   },
   "outputs": [],
   "source": [
    "# Importiere die .csv-Dateien mit den EMG-Daten der einzelnen Subjekte und den einzelnen Labels (Gesten)\n",
    "# Dabei gilt folgendes Schema, siehe auch Bilder im Ordner \"Gestenbilder\"\n",
    "# Label 0 --> Papier\n",
    "# Label 1 --> Stein\n",
    "# Label 2 --> Schere\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Aufnahme der Einzelgesten                                  #\n",
    "# Datentyp: Python List                                      #\n",
    "##############################################################\n",
    "subject1 = []\n",
    "subject2 = []\n",
    "subject3 = []\n",
    "subject4 = []\n",
    "subject5 = []\n",
    "subject6 = []\n",
    "subject7 = []\n",
    "subject8 = []\n",
    "subject9 = []\n",
    "subject10 = []\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Baue daraus den Dateipfad und lese Datei ein               #\n",
    "# Schreibe Nummer der Geste in entsprechende Spalte          #\n",
    "##############################################################\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject1.append(pd.read_csv('./Datasets/Concatenated/S1L' + label + '.csv', sep=',', header=None))     \n",
    "    subject1[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject2.append(pd.read_csv('./Datasets/Concatenated/S2L' + label + '.csv', sep=',', header=None))     \n",
    "    subject2[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject3.append(pd.read_csv('./Datasets/Concatenated/S3L' + label + '.csv', sep=',', header=None))     \n",
    "    subject3[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject4.append(pd.read_csv('./Datasets/Concatenated/S4L' + label + '.csv', sep=',', header=None))     \n",
    "    subject4[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject5.append(pd.read_csv('./Datasets/Concatenated/S5L' + label + '.csv', sep=',', header=None))     \n",
    "    subject5[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject6.append(pd.read_csv('./Datasets/Concatenated/S6L' + label + '.csv', sep=',', header=None))     \n",
    "    subject6[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject7.append(pd.read_csv('./Datasets/Concatenated/S7L' + label + '.csv', sep=',', header=None))     \n",
    "    subject7[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject8.append(pd.read_csv('./Datasets/Concatenated/S8L' + label + '.csv', sep=',', header=None))     \n",
    "    subject8[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject9.append(pd.read_csv('./Datasets/Concatenated/S9L' + label + '.csv', sep=',', header=None))     \n",
    "    subject9[int(label)][8] = int(label)\n",
    "\n",
    "for label in ['0', '1', '2']:\n",
    "    subject10.append(pd.read_csv('./Datasets/Concatenated/S10L' + label + '.csv', sep=',', header=None))     \n",
    "    subject10[int(label)][8] = int(label)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Schreibe alle Sub-Dataframes in ein gesamtes Dataframe     #\n",
    "# Datentyp: Pandas Dataframe                                 #\n",
    "##############################################################\n",
    "subject1 = pd.concat([df for df in subject1], axis=0).reset_index(drop=True)\n",
    "subject2 = pd.concat([df for df in subject2], axis=0).reset_index(drop=True)\n",
    "subject3 = pd.concat([df for df in subject3], axis=0).reset_index(drop=True)\n",
    "subject4 = pd.concat([df for df in subject4], axis=0).reset_index(drop=True)\n",
    "subject5 = pd.concat([df for df in subject5], axis=0).reset_index(drop=True)\n",
    "subject6 = pd.concat([df for df in subject6], axis=0).reset_index(drop=True)\n",
    "subject7 = pd.concat([df for df in subject7], axis=0).reset_index(drop=True)\n",
    "subject8 = pd.concat([df for df in subject8], axis=0).reset_index(drop=True)\n",
    "subject9 = pd.concat([df for df in subject9], axis=0).reset_index(drop=True)\n",
    "subject10 = pd.concat([df for df in subject10], axis=0).reset_index(drop=True)\n",
    "\n",
    "sub_1 = pd.DataFrame(subject1)\n",
    "sub_5 = pd.concat([subject1, subject2, subject3, subject4, subject5])\n",
    "sub_10 = pd.concat([subject1, subject2, subject3, subject4, subject5, subject6, subject7, subject8, subject9, subject10])\n",
    "\n",
    "sub_10.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ▶️ Umwandlung in geeignete Datenstrukturen\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, n, reminder = None):\n",
    "    # Get number of rows in DataFrame\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Initialize list to hold the split DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Calculate the number of DataFrames\n",
    "    num_dfs = total_rows // n\n",
    "    extra_rows = total_rows % n\n",
    "    \n",
    "    if reminder == 'first' and extra_rows > 0:\n",
    "        # Size of first chunk is equal to reminder. Remaining chunks are same size.\n",
    "        dfs.append(df.iloc[:n + extra_rows])\n",
    "        start_idx = n + extra_rows\n",
    "        for _ in range(1, num_dfs):\n",
    "            dfs.append(df.iloc[start_idx:start_idx + n])\n",
    "            start_idx += n\n",
    "    elif reminder == 'last' and extra_rows > 0:\n",
    "        # Last chunk consists of remining rows. The other chunks are the same size.\n",
    "        for i in range(num_dfs):\n",
    "            dfs.append(df.iloc[i*n:(i+1)*n])\n",
    "        # Add extra rows to the last chunk\n",
    "        dfs.append(df.iloc[num_dfs*n:])\n",
    "    elif reminder == 'spread':\n",
    "        # Evenly spread extra rows across the first few chunks\n",
    "        for i in range(num_dfs + (1 if extra_rows > 0 else 0)):\n",
    "            size = n + (1 if i < extra_rows else 0)\n",
    "            dfs.append(df.iloc[i*size:min((i+1)*size, total_rows)])\n",
    "    else:\n",
    "        if extra_rows > 0:\n",
    "            raise ValueError(f\"DataFrame Länge ist nicht teilbar durch {n}. Bitte nutze den 'reminder' Parameter.\")\n",
    "        # If remainder is None and the DataFrame is perfectly divisible\n",
    "        for i in range(num_dfs):\n",
    "            dfs.append(df.iloc[i*n:(i+1)*n])\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Einzelwerte der Labels auslesen                            #\n",
    "# Datentyp Pandas Dataframe                                  #\n",
    "##############################################################\n",
    "Labels = sub_5[sub_5.columns[-1]]\n",
    " \n",
    "\n",
    "##############################################################\n",
    "# Klassen aus den Labels auslesen                            #\n",
    "# Datentyp Numpy Array                                       #\n",
    "##############################################################\n",
    "classes = np.unique(Labels)\n",
    "print(\"Classes Shape:\",classes.shape)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Sensorwerte der Gesten auslesen                            #\n",
    "# Datentyp Pandas Dataframe                                  #\n",
    "##############################################################\n",
    "Features = sub_5.drop(8, axis=1).copy()\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Zerteile Gesamtdaten in Einzelpakete mit jeweils 16 Zeilen #\n",
    "# Datentyp Python List                                       #\n",
    "##############################################################\n",
    "Features = split_dataframe(Features, 8, 'last')\n",
    "Labels = split_dataframe(Labels, 8, 'last')\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Entferne letztes Tupel aus Listen                          #\n",
    "# Bei ungerader Anzahl Tupeln hat das letzte Listenelement   #\n",
    "# eine abweichende Anzahl an Datenpunkten --> Problem        #\n",
    "# Datentyp Python List                                       #\n",
    "##############################################################\n",
    "l_features = len(Features)-1\n",
    "Features = Features[:l_features]\n",
    "\n",
    "l_labels = len(Labels)-1\n",
    "Labels = Labels[:l_labels]\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Konvertiere die Listen in Numpy-Arrays als Float32         #\n",
    "# Dann eine Dimension hinzu als Channel                      #\n",
    "# Datentyp Numpy-Array                                       #\n",
    "##############################################################\n",
    "Features = np.array(Features).astype('float32')\n",
    "Features = Features.reshape(-1,8)\n",
    "\n",
    "#sc = MinMaxScaler(feature_range=(-30,30))\n",
    "#sc = MinMaxScaler()\n",
    "#sc = StandardScaler()\n",
    "\n",
    "#Features = sc.fit_transform(Features)\n",
    "\n",
    "Features = Features.reshape(-1,8,8)\n",
    "Features = np.expand_dims(Features, -1)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Zuerst alle Label-Arrays nach 1D konvertieren              #\n",
    "# Dann eine Dimension hinzu als Channel                      #\n",
    "# Wichtig für das Convolution Neuronal Network !!            #\n",
    "##############################################################\n",
    "Labels = np.array(Labels).astype('int')\n",
    "Labels = Labels[:,1]\n",
    "Labels = np.expand_dims(Labels, -1)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Gebe die Form des Image-Vectors aus                        #\n",
    "##############################################################\n",
    "print(\"Features Shape: \", Features.shape)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Gebe die Form des Label-Vectors aus                        #\n",
    "##############################################################\n",
    "print(\"Labels Shape:\", Labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqtb_0qsqcBe"
   },
   "source": [
    "### ▶️ Datensatz aufteilen in Training- und Test-Daten\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkE318CuqruZ",
    "outputId": "80b11002-44b4-4019-c983-05a729ffe78c"
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Einkommentieren für den Kaggle-Datensatz                   #\n",
    "# Fixwert 4                                                  #\n",
    "##############################################################\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Fix 64 Sensorwerte pro Zeile --> (16, 64, 1)               #\n",
    "##############################################################\n",
    "input_shape = (8, 8, 1)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Aufteilung der Datensätze in Training- und Testdaten       #\n",
    "# Test-Size --> prozentualer Anteil von Gesamtdaten          #\n",
    "# Random-State --> Zufallsgrad der Durchmischung             #\n",
    "##############################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features, Labels, test_size = 0.15, random_state = 100) # type: ignore\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.15, random_state=100)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Status der Datensätze ausgeben                             #\n",
    "##############################################################\n",
    "print(\"X_train Vectorform:\", X_train.shape)\n",
    "print(\"X_test Vectorform:\", X_test.shape)\n",
    "print(\"X_validate Vectorform:\", X_val.shape)\n",
    "print(\"y_train Vectorform:\", y_train.shape)\n",
    "print(\"y_validate Vectorform:\", y_val.shape)\n",
    "print(\"y_test Vectorform:\", y_test.shape)\n",
    "print()\n",
    "print(Features.shape[0], \"Gesamtwerte\")\n",
    "print(X_train.shape[0], \"Trainingswerte\")\n",
    "print(X_test.shape[0], \"Testwerte\")\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Klassenvektor in Matrize konvertieren (One-Hot-Encoding)   #\n",
    "##############################################################\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▶️ Modellarchitektur\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Hyperparameter                                             #\n",
    "##############################################################\n",
    "batch_size = 32\n",
    "epochs = 16\n",
    "\n",
    "initializer='glorot_uniform'\n",
    "padding='same'\n",
    "\n",
    "##############################################################\n",
    "# Modell erstellen                                           #\n",
    "##############################################################\n",
    "model = keras.Sequential()\n",
    "model.add(layers.InputLayer(input_shape))\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(64, (2,2), strides=(2,2) ,padding = padding, activation='relu', input_shape=input_shape, kernel_initializer=initializer))\n",
    "model.add(layers.MaxPool2D(1,1))\n",
    "\n",
    "model.add(layers.Conv2D(64, (2,2), strides=(2,2) , padding = padding, activation='relu'))\n",
    "model.add(layers.MaxPool2D(1,1))\n",
    "\n",
    "model.add(layers.Reshape((-1,)))\n",
    "model.add(layers.Dense(32, activation= 'relu'))\n",
    "\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Modellparameter ausgeben\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell kompilieren\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = 'categorical_crossentropy',    \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Parameter und Daten an Modell übergeben\n",
    "# Training durchführen\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=batch_size,    \n",
    "    epochs=epochs,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▶️ Exportiere das Modell nach C++\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Konvertiere Modell in Format für Headerfile                #\n",
    "# Inhalt ab/inklusive der Zeile \"#pragma once\"               #\n",
    "# in eine neue Datei mit Namen \"Model.h\" kopieren            #\n",
    "##############################################################\n",
    "from everywhereml.code_generators.tensorflow import convert_model\n",
    "c_header = convert_model(model, X_train, y_train, model_name='FCNN')\n",
    "print(c_header)\n",
    "\n",
    "# from eloquent_tensorflow import convert_model\n",
    "# print(convert_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▶️ Ausgabe der Verlustfunktion für Training und Validation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKYDX19s1oH3"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'r.', label='Trainingsverlust')\n",
    "plt.plot(epochs, val_loss, 'y', label='Validationsverlust')\n",
    "plt.title('Verlust Training und Validation')\n",
    "plt.xlabel('Epochen')\n",
    "plt.ylabel('Verlust')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('./Images/Loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▶️ Ausgabe der Genauigkeit für Training und Validation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, acc, 'r.', label='Trainingsgenauigkeit')\n",
    "plt.plot(epochs, val_acc, 'y', label='Validationsgenauigkeit')\n",
    "plt.title('Genauigkeit Training und Validation')\n",
    "plt.xlabel('Epochen')\n",
    "plt.ylabel('Genauigkeit')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('./Images/Accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▶️ Modell auf Testdaten anwenden und evaluieren\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ▶️ Metriken zur Beurteilung der Modellqualität \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_arg=np.argmax(y_test,axis=1)\n",
    "y_pred = np.argmax(model.predict(X_test),axis=1)\n",
    " \n",
    "cm = confusion_matrix(y_test_arg, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Greens', cbar=False)\n",
    "plt.xlabel(\"Vorhersagen\")\n",
    "plt.ylabel(\"Geste\")\n",
    "plt.title(\"Konfusionsmatrix\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test_arg, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNuAXIB7ixK/3gtqh86Wo2I",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
